"Hello and welcome. Today, I want to demonstrate a style transfer workflow using ComfyUI."
"And I know that I've been working on fast style transfer node. First of all, you need"
"to cancel your ComfyUI runtime. And we need to copy and paste these commands into your"
"terminal. I will provide a link in the description. This is just a workaround. So you see there"
"are conflicts with protobuf versions conflicting."
"Okay."
"Anyway, we'll run Comfy again using Python main.py. And hopefully this will be resolved."
"So we can refresh the window. Okay. It seems that the fast style transfer node has been"
"loaded successfully. You can find this node in my ComfyUI node toolbox."
"Okay."
"And you need to download this as a zip or copy git clone to your ComfyUI directory,"
"which in my case is right here. So there's a bunch of custom nodes already. But you need"
"to paste this whole folder into the custom nodes directory."
"Okay."
"There's also the fast style transfer. This is the original fast style transfer repo."
"The code is actually based on the original one from TensorFlow tutorial, from the TensorFlow"
"tutorial."
"But I've modified it a bit. And you can also download this. And you can see that the code"
"is already there. So you can load this onto your machine and run it locally. So git clone."
"And set up the conda environment with these two commands. And then you should be able"
"to run this on your machine."
"So let's go back into ComfyUI. We have two images. The one is the input image and the"
"other one is the style reference image."
"Okay."
"We can see that this is a bit of low quality. But let's try and run this, actually."
"So I'm doing a bit of color conversion here. But the main backbone of the effect is the"
"fast style transfer."
"And then we're moving into the VAE encoder."
"Okay."
"Okay."
"And I'm also injecting latent noise into the latent space provided with a conditioning"
"clip text encoder."
"So I'm describing this image above, namely the style image. So you can use ChatGPT or"
"you can use your own words to describe."
"What this is doing or what style this actually is."
"Be careful with the noise style strength. You need to play around with that as well."
"But usually injecting less noise will be more coherent to the original image. And also the"
"CFG classifier free guidance."
"Put in the ponch part of the result."
"So the CFG classifier free guidance will define how much weight is given to the text encoder in that"
"sense of how much effect the text description will affect the final output."
"Or how strong the style image will affect this input image."
"I hope this is clear."
"Okay."
"And finally, I'm also upscaling the output, final output."
"Now let's just run this."
"So we can see it's generating the first images."
"They're pretty small, I think."
"And this is using a bit more VRAM now,"
"since it's also encoding these images again"
"into the latent space."
"I will fast forward time here until the generation is done."
"OK, so the images have completed."
"We can see them down here."
"This is the effect."
"This image has quite low resolution."
"Let's try with another image."
"If we go into our folder again, I think this one is interesting."
"By the way, in Comfy, you can shift-select nodes"
"and then align them."
"Change this prompt."
"Let's ask Ollama."
"Ollama, run Ollama 3.1, I think."
"Modify prompt."
"Modify."
"from the match."
"Spread of shield."
"Actually, we're having a portrait here."
"So what I'll do is I'll just do a portrait."
"OK."
"So ChatGPT is much better."
"It's much better with description of stable diffusion prompts."
"But it's running locally."
"And it's free."
"And it doesn't send any data to OpenAI for them to gather,"
"and so on, et cetera."
"So let's paste this in here."
"Probably."
"OK."
"So let's run this again."
"Queue up."
"We'll use a random seed."
"I think this is interesting."
"I will modify time again, and I will see you when this is done."
"OK."
"So the generations have finished."
"Interesting, but not quite what we want."
"Think something is wrong."
"Let's try a different sampler."
"We can try to load this quantized model, which"
"will require less VRAM."
"I'm not particularly good in maths, but set this to 0.3."
"We're rounding this value, so 0.4."
"Subtract."
"Subtracting the two latents."
"And here, I will also modify time again."
"And I will see you when this has finished rendering."
"OK."
"So the generations are done."
"And we're having pretty much the same results as before."
"How can I improve this?"
"Well, first of all, we can."
"We can copy the complete prompt here and paste it into the clip conditioning."
"So just get some weight here."
"So something."
"OK."
"Let's see."
"OK."
"OK."
"So this is what we're going to do."
"It's happening with the sampling."
"Just want to see if we bypass this node."
"I'll generate again and fast forward the time when this is done."
"Not quite what we want."
"But I think it's interesting."
"So I think the latent noise is doing some funky stuff with the image, with the output"
"image."
"So feel free to bypass all of these nodes."
"And we'll run the queue again."
"And I will see you when this is done."
"OK."
"And I will fast forward time yet another time."
"OK."
"So this is bypassing the noise injection part and upscaling the image."
"So we still get artifacts in there."
"Anyway."
"I hope you found this introduction demo stream video interesting."
"I think I like this one."
"I'll save this to my downloads folder."
"And I hope you learned something or found it interesting."
"Links will be in the description."
"Also, I have the ComfyUI."
"Try to find this Workflow Suite, which will host these workflows."
"And I will also try to update the GitHub README with the appropriate workflow."
"I will put this in here."
"And also link that up in the YouTube description."
"So that's it."
"Thanks for watching."
"Hope to see you in the next one."
"Goodbye."

